{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e1f9a20-70b5-4195-8991-111190cb234e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f10bbe9-8e9d-4d56-a761-0a9d2af7c4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ae6f98a-5eb7-412e-8dfb-0e69227cf773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1768b1ce-cf8c-42ea-98e6-bbdef71f90c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Read networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8484615e-7f81-4051-8c89-049931979170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sknetwork\n",
    "from sknetwork.clustering import Louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "685306bf-83ae-45ec-bbf3-492ffb1d539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDict = dict()\n",
    "for cat in ['western','russian']:\n",
    "    with open(f'visualizations/ngramsNetwork/{cat}ngramNet5_8-5_w_nodeandlinkpolarity.json','r') as json_file:\n",
    "        dataDict[cat] = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "18611465-4db9-4e67-a4e6-312934693577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1032 41867\n",
      "1446 75849\n"
     ]
    }
   ],
   "source": [
    "for cat in dataDict:\n",
    "    print(len(dataDict[cat]['nodes']),len(dataDict[cat]['links']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "56ebb44a-b113-4273-9a0f-63e8857be3d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([n for n in dataDict['western']['nodes'] if n['occurrences'] >= 600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d053d096-1d11-4412-84a8-1d17b2c357b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([n for n in dataDict['russian']['nodes'] if n['occurrences'] >= 750])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d5f07448-5b0b-4131-9aa1-52df85149f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 1138, 'target': 184, 'value': 1, 'polarityScore': -0.318}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDict['russian']['links'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a5a5148d-2c1f-4d14-b237-bd139a84dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "smallJson = {'nodes' : [], 'links': []}\n",
    "j = 0\n",
    "newNodes = set()\n",
    "old2new = dict()\n",
    "for i, n in enumerate(dataDict['russian']['nodes']):\n",
    "    if n['occurrences'] >= 750:\n",
    "        old2new[i] = j\n",
    "        smallJson['nodes'].append(n)\n",
    "        newNodes.add(i)\n",
    "        j += 1\n",
    "for l in dataDict['russian']['links']:\n",
    "    s = l['source']\n",
    "    t = l['target']\n",
    "    if s in newNodes and t in newNodes:\n",
    "        newLink = l.copy()\n",
    "        newLink['source'] = old2new[s]\n",
    "        newLink['target'] = old2new[t]\n",
    "        smallJson['links'].append(newLink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "621f280e-efe9-4412-b98c-dd5a9f894173",
   "metadata": {},
   "outputs": [],
   "source": [
    "numNodes = len(smallJson['nodes'])\n",
    "adjArray = np.zeros((numNodes, numNodes), dtype=int)\n",
    "for l in smallJson['links']:\n",
    "    i = l['source']\n",
    "    j = l['target']\n",
    "    adjArray[i,j] = l['value']\n",
    "    adjArray[j,i] = l['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e1839246-7fa5-4678-bb54-29d79eb7899d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "louvain = Louvain()\n",
    "labels = louvain.fit_transform(adjArray)\n",
    "len(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e8612e22-5ca9-42ec-b597-4365cc0000a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 2, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 3, 1, 3,\n",
       "       0, 0, 2, 0, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "louvain.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dd2b5932-b959-413e-b14b-894c16f398f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, n in enumerate(smallJson['nodes']):\n",
    "    n['group'] = int(louvain.labels_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c40f58fb-0cc3-4f38-86de-8809fa47417f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote json\n"
     ]
    }
   ],
   "source": [
    "with open(f'visualizations/adjmatrix/sample_russian.json', 'w', encoding ='ascii') as json_file:\n",
    "    json.dump(smallJson, json_file, ensure_ascii = True)\n",
    "print('wrote json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdac54a9-39a6-4884-8d76-195a1a42558a",
   "metadata": {},
   "source": [
    "## Topic-specific matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "27f3e429-d558-44a2-94ab-df41f04d6c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "843d429f-1439-47e5-87a2-fb4bd4b4a63a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.9 s, sys: 1.9 s, total: 16.8 s\n",
      "Wall time: 23.9 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>newsOutlet</th>\n",
       "      <th>category</th>\n",
       "      <th>dateSeen</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>language</th>\n",
       "      <th>sourceCountry</th>\n",
       "      <th>sentText</th>\n",
       "      <th>sentIndexInText</th>\n",
       "      <th>sentTopicID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>21</td>\n",
       "      <td>apnews.com</td>\n",
       "      <td>western</td>\n",
       "      <td>2021-10-01 07:00:00</td>\n",
       "      <td>https://apnews.com/article/2020-tokyo-olympics...</td>\n",
       "      <td>Poland grants visa to Belarus Olympian who fea...</td>\n",
       "      <td>English</td>\n",
       "      <td>United States</td>\n",
       "      <td>TOKYO (AP) â€” Poland granted a visa Monday to a...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>21</td>\n",
       "      <td>apnews.com</td>\n",
       "      <td>western</td>\n",
       "      <td>2021-10-01 07:00:00</td>\n",
       "      <td>https://apnews.com/article/2020-tokyo-olympics...</td>\n",
       "      <td>Poland grants visa to Belarus Olympian who fea...</td>\n",
       "      <td>English</td>\n",
       "      <td>United States</td>\n",
       "      <td>The current standoff apparently began after Ts...</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>21</td>\n",
       "      <td>apnews.com</td>\n",
       "      <td>western</td>\n",
       "      <td>2021-10-01 07:00:00</td>\n",
       "      <td>https://apnews.com/article/2020-tokyo-olympics...</td>\n",
       "      <td>Poland grants visa to Belarus Olympian who fea...</td>\n",
       "      <td>English</td>\n",
       "      <td>United States</td>\n",
       "      <td>The runner said on her Instagram account that ...</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>21</td>\n",
       "      <td>apnews.com</td>\n",
       "      <td>western</td>\n",
       "      <td>2021-10-01 07:00:00</td>\n",
       "      <td>https://apnews.com/article/2020-tokyo-olympics...</td>\n",
       "      <td>Poland grants visa to Belarus Olympian who fea...</td>\n",
       "      <td>English</td>\n",
       "      <td>United States</td>\n",
       "      <td>The runner was then apparently hustled to the ...</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>21</td>\n",
       "      <td>apnews.com</td>\n",
       "      <td>western</td>\n",
       "      <td>2021-10-01 07:00:00</td>\n",
       "      <td>https://apnews.com/article/2020-tokyo-olympics...</td>\n",
       "      <td>Poland grants visa to Belarus Olympian who fea...</td>\n",
       "      <td>English</td>\n",
       "      <td>United States</td>\n",
       "      <td>In a filmed message distributed on social medi...</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  newsOutlet category             dateSeen  \\\n",
       "443     21  apnews.com  western  2021-10-01 07:00:00   \n",
       "445     21  apnews.com  western  2021-10-01 07:00:00   \n",
       "446     21  apnews.com  western  2021-10-01 07:00:00   \n",
       "447     21  apnews.com  western  2021-10-01 07:00:00   \n",
       "448     21  apnews.com  western  2021-10-01 07:00:00   \n",
       "\n",
       "                                                   url  \\\n",
       "443  https://apnews.com/article/2020-tokyo-olympics...   \n",
       "445  https://apnews.com/article/2020-tokyo-olympics...   \n",
       "446  https://apnews.com/article/2020-tokyo-olympics...   \n",
       "447  https://apnews.com/article/2020-tokyo-olympics...   \n",
       "448  https://apnews.com/article/2020-tokyo-olympics...   \n",
       "\n",
       "                                                 title language  \\\n",
       "443  Poland grants visa to Belarus Olympian who fea...  English   \n",
       "445  Poland grants visa to Belarus Olympian who fea...  English   \n",
       "446  Poland grants visa to Belarus Olympian who fea...  English   \n",
       "447  Poland grants visa to Belarus Olympian who fea...  English   \n",
       "448  Poland grants visa to Belarus Olympian who fea...  English   \n",
       "\n",
       "     sourceCountry                                           sentText  \\\n",
       "443  United States  TOKYO (AP) â€” Poland granted a visa Monday to a...   \n",
       "445  United States  The current standoff apparently began after Ts...   \n",
       "446  United States  The runner said on her Instagram account that ...   \n",
       "447  United States  The runner was then apparently hustled to the ...   \n",
       "448  United States  In a filmed message distributed on social medi...   \n",
       "\n",
       "     sentIndexInText  sentTopicID  \n",
       "443                0           -1  \n",
       "445                2           -1  \n",
       "446                3           -1  \n",
       "447                4           -1  \n",
       "448                5           -1  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sdf = pd.read_excel('data/topic_model_data/fullSentDataFrame_8-3.xlsx',index_col=0)\n",
    "sdf = sdf.drop_duplicates(['sentText'])\n",
    "sdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "eb079a4f-042b-4f8f-92ac-47782441dac9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c8d69212424748806b43f1fa85f3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.2.2.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-08 10:02:56 INFO: Downloading default packages for language: en (English)...\n",
      "2022-08-08 10:02:59 INFO: File exists: /Users/paigelee/stanza_resources/en/default.zip.\n",
      "2022-08-08 10:03:05 INFO: Finished downloading models and saved to /Users/paigelee/stanza_resources.\n",
      "2022-08-08 10:03:05 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2022-08-08 10:03:05 INFO: Use device: cpu\n",
      "2022-08-08 10:03:05 INFO: Loading: tokenize\n",
      "2022-08-08 10:03:05 INFO: Loading: pos\n",
      "2022-08-08 10:03:06 INFO: Loading: lemma\n",
      "2022-08-08 10:03:06 INFO: Loading: depparse\n",
      "2022-08-08 10:03:07 INFO: Loading: sentiment\n",
      "2022-08-08 10:03:08 INFO: Loading: ner\n",
      "2022-08-08 10:03:09 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "18ff55c1-1b36-4668-83bb-c7d1e3ec92f9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/paigelee/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/paigelee/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.vader import SentiText\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "engStopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc974f27-d559-4fe5-b9c7-3de513b387cf",
   "metadata": {},
   "source": [
    "### Pre-compute lemmas and polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "711436c9-f0b1-42fb-8ce0-fa7bfeb59cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/53349 recs parsed by NLP\n",
      "1000/53349 recs parsed by NLP\n",
      "2000/53349 recs parsed by NLP\n",
      "3000/53349 recs parsed by NLP\n",
      "4000/53349 recs parsed by NLP\n",
      "5000/53349 recs parsed by NLP\n",
      "6000/53349 recs parsed by NLP\n",
      "7000/53349 recs parsed by NLP\n",
      "8000/53349 recs parsed by NLP\n",
      "9000/53349 recs parsed by NLP\n",
      "10000/53349 recs parsed by NLP\n",
      "11000/53349 recs parsed by NLP\n",
      "12000/53349 recs parsed by NLP\n",
      "13000/53349 recs parsed by NLP\n",
      "14000/53349 recs parsed by NLP\n",
      "15000/53349 recs parsed by NLP\n",
      "16000/53349 recs parsed by NLP\n",
      "17000/53349 recs parsed by NLP\n",
      "18000/53349 recs parsed by NLP\n",
      "19000/53349 recs parsed by NLP\n",
      "20000/53349 recs parsed by NLP\n",
      "21000/53349 recs parsed by NLP\n",
      "22000/53349 recs parsed by NLP\n",
      "23000/53349 recs parsed by NLP\n",
      "24000/53349 recs parsed by NLP\n",
      "25000/53349 recs parsed by NLP\n",
      "26000/53349 recs parsed by NLP\n",
      "27000/53349 recs parsed by NLP\n",
      "28000/53349 recs parsed by NLP\n",
      "29000/53349 recs parsed by NLP\n",
      "30000/53349 recs parsed by NLP\n",
      "31000/53349 recs parsed by NLP\n",
      "32000/53349 recs parsed by NLP\n",
      "33000/53349 recs parsed by NLP\n",
      "34000/53349 recs parsed by NLP\n",
      "35000/53349 recs parsed by NLP\n",
      "36000/53349 recs parsed by NLP\n",
      "37000/53349 recs parsed by NLP\n",
      "38000/53349 recs parsed by NLP\n",
      "39000/53349 recs parsed by NLP\n",
      "40000/53349 recs parsed by NLP\n",
      "41000/53349 recs parsed by NLP\n",
      "42000/53349 recs parsed by NLP\n",
      "43000/53349 recs parsed by NLP\n",
      "44000/53349 recs parsed by NLP\n",
      "45000/53349 recs parsed by NLP\n",
      "46000/53349 recs parsed by NLP\n",
      "47000/53349 recs parsed by NLP\n",
      "48000/53349 recs parsed by NLP\n",
      "49000/53349 recs parsed by NLP\n",
      "50000/53349 recs parsed by NLP\n",
      "51000/53349 recs parsed by NLP\n",
      "52000/53349 recs parsed by NLP\n",
      "53000/53349 recs parsed by NLP\n"
     ]
    }
   ],
   "source": [
    "allSentRecords = sdf.to_dict('records')\n",
    "\n",
    "for i, rec in enumerate(allSentRecords):\n",
    "    if i % 1000 == 0:\n",
    "        print(f'{i}/{len(allSentRecords)} recs parsed by NLP')\n",
    "    docText = rec['sentText']\n",
    "    nlpdoc = nlp(docText)\n",
    "\n",
    "    ss = sid.polarity_scores(docText)\n",
    "    polarityScore = ss['pos'] - ss['neg']\n",
    "\n",
    "    wordList = [w.lemma.lower() for w in nlpdoc.iter_words() if w.text.isalpha() and w.text.lower() not in engStopwords]\n",
    "    rec['polarityScore'] = polarityScore\n",
    "    rec['wordList'] = wordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1f188c36-b5bd-4696-ad82-1e80553c5ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('Sentence_Data_with_Polarity_and_Lemma_Lists.pickle', 'wb') as handle:\n",
    "#     pickle.dump(allSentRecords, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "253d2ce2-2ba5-482f-b33e-cc63ca101683",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Sentence_Data_with_Polarity_and_Lemma_Lists.pickle', 'rb') as handle:\n",
    "    sentenceRecs = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f275cb4f-4330-4e48-8c55-7ee71ba0ea14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': 250,\n",
       " 'newsOutlet': 'apnews.com',\n",
       " 'category': 'western',\n",
       " 'dateSeen': datetime.datetime(2022, 1, 26, 8, 0),\n",
       " 'url': 'https://apnews.com/article/russia-ukraine-russia-malaysia-europe-moscow-ff98b54807f2f6c74902504f3ba0db03',\n",
       " 'title': 'Netherlands , Ukraine take Russia to European rights court',\n",
       " 'language': 'English',\n",
       " 'sourceCountry': 'United States',\n",
       " 'sentText': 'In a statement, Dutch Foreign Minister Wopke Hoekstra called Wednesdayâ€™s hearing â€œan important step in the Netherlandsâ€™ pursuit of justice for the victims and their next of kin.',\n",
       " 'sentIndexInText': 16,\n",
       " 'sentTopicID': 14,\n",
       " 'polarityScore': 0.09200000000000001,\n",
       " 'wordList': ['statement',\n",
       "  'dutch',\n",
       "  'foreign',\n",
       "  'minister',\n",
       "  'wopke',\n",
       "  'hoekstra',\n",
       "  'call',\n",
       "  'wednesday',\n",
       "  'hearing',\n",
       "  'important',\n",
       "  'step',\n",
       "  'netherlands',\n",
       "  'pursuit',\n",
       "  'justice',\n",
       "  'victim',\n",
       "  'next',\n",
       "  'kin']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(sentenceRecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f54c457-9ccd-4310-aea4-689de29742a5",
   "metadata": {},
   "source": [
    "## Run co-occurrence code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1af88ca6-1686-48af-8b50-0d352d5bd49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8f1c84a-b4a3-4675-a1e6-b9eb22ebc1e5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 0 nodes and 0 links\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 0 nodes and 0 links\n",
      "0/835 recs parsed by NLP\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 8 nodes and 24 links\n",
      "0/796 recs parsed by NLP\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 9 nodes and 34 links\n",
      "0/267 recs parsed by NLP\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 2 nodes and 1 links\n",
      "0/410 recs parsed by NLP\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 3 nodes and 3 links\n",
      "0/470 recs parsed by NLP\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 1 nodes and 0 links\n",
      "0/622 recs parsed by NLP\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 1 nodes and 0 links\n",
      "0/114 recs parsed by NLP\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 0 nodes and 0 links\n",
      "0/227 recs parsed by NLP\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 1 nodes and 0 links\n",
      "0/611 recs parsed by NLP\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 7 nodes and 20 links\n",
      "0/585 recs parsed by NLP\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 6 nodes and 15 links\n",
      "0/771 recs parsed by NLP\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 15 nodes and 82 links\n",
      "0/1318 recs parsed by NLP\n",
      "1000/1318 recs parsed by NLP\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 28 nodes and 257 links\n",
      "0/987 recs parsed by NLP\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 18 nodes and 129 links\n",
      "0/1067 recs parsed by NLP\n",
      "1000/1067 recs parsed by NLP\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 19 nodes and 133 links\n",
      "0/798 recs parsed by NLP\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 13 nodes and 70 links\n",
      "0/996 recs parsed by NLP\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 14 nodes and 84 links\n",
      "0/386 recs parsed by NLP\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 8 nodes and 24 links\n",
      "0/298 recs parsed by NLP\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 10 nodes and 39 links\n",
      "0/194 recs parsed by NLP\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 3 nodes and 3 links\n",
      "0/427 recs parsed by NLP\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 8 nodes and 25 links\n",
      "0/175 recs parsed by NLP\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 0 nodes and 0 links\n",
      "0/1305 recs parsed by NLP\n",
      "1000/1305 recs parsed by NLP\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 23 nodes and 163 links\n",
      "0/584 recs parsed by NLP\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 5 nodes and 9 links\n",
      "0/159 recs parsed by NLP\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 0 nodes and 0 links\n",
      "0/1519 recs parsed by NLP\n",
      "1000/1519 recs parsed by NLP\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 24 nodes and 216 links\n",
      "0/1845 recs parsed by NLP\n",
      "1000/1845 recs parsed by NLP\n",
      "stopping the russian count at 150006words\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 20 nodes and 148 links\n",
      "0/1943 recs parsed by NLP\n",
      "1000/1943 recs parsed by NLP\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 30 nodes and 303 links\n",
      "0/3430 recs parsed by NLP\n",
      "stopping the russian count at 150026words\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 0 nodes and 0 links\n",
      "0/2653 recs parsed by NLP\n",
      "stopping the western count at 150008words\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 0 nodes and 0 links\n",
      "0/2862 recs parsed by NLP\n",
      "stopping the russian count at 150032words\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 0 nodes and 0 links\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# getallSentRecordsccurrence connections\n",
    "jsonDict = dict()\n",
    "\n",
    "thresh = 100\n",
    "n = 5\n",
    "\n",
    "totalWordCt = {\n",
    "    'western' : 0,\n",
    "    'russian' : 0\n",
    "}\n",
    "topicDict = dict()\n",
    "for topicID in range(-1,15):\n",
    "    topicDict[topicID] = dict()\n",
    "    for cat in ['western','russian']:\n",
    "        networkJson = dict()\n",
    "        networkJson['nodes'] = []\n",
    "        networkJson['links'] = []\n",
    "\n",
    "        curatedNodes = set()\n",
    "        linkCounter = dict()\n",
    "        keywordCounter = dict()\n",
    "        polarityOfNode = dict()\n",
    "\n",
    "        windowList = []\n",
    "        records = []\n",
    "        for d in sentenceRecs:\n",
    "            if d['category'] == cat and d['sentTopicID'] != -1 and d['sentTopicID'] == topicID:\n",
    "                records.append(d)\n",
    "\n",
    "        for i, rec in enumerate(records):\n",
    "            if i % 1000 == 0:\n",
    "                print(f'{i}/{len(records)} recs parsed by NLP')\n",
    "\n",
    "            polarityScore = rec['polarityScore']\n",
    "            wordList = rec['wordList']\n",
    "\n",
    "            for windowIdx in range(len(wordList) // n + len(wordList) % n):\n",
    "                startIdx = windowIdx\n",
    "                endIdx = windowIdx+random.choice(list(range(1,n+1)))\n",
    "                wordWindow = wordList[startIdx:endIdx]\n",
    "                tokens = []\n",
    "                for wordToken in wordWindow:\n",
    "                    formattedToken = wordToken\n",
    "                    tokens.append(formattedToken)\n",
    "                windowList.append((tokens, polarityScore))\n",
    "            for t1 in wordList:\n",
    "                keywordCounter.setdefault(t1, 0)\n",
    "                keywordCounter[t1] += 1\n",
    "                polarityOfNode.setdefault(t1, [])\n",
    "                polarityOfNode[t1].append(polarityScore)\n",
    "                totalWordCt[cat] += 1\n",
    "            if totalWordCt[cat] >= 150000:\n",
    "                print(f'stopping the {cat} count at {totalWordCt[cat]}words')\n",
    "                break\n",
    "        print('\\tdone nlping docs')\n",
    "        sufficientNodes = set()\n",
    "        for key in keywordCounter:\n",
    "            if keywordCounter[key] >= thresh:\n",
    "                sufficientNodes.add(key)\n",
    "        print('\\tfound sufficient recs')\n",
    "        polarityOfLinks= dict()\n",
    "\n",
    "        for tokens, polarityScore in windowList:\n",
    "            s = set(tokens)\n",
    "            subsets = [tuple(i) for i in itertools.combinations(s, 2)]\n",
    "            for t1, t2 in subsets:\n",
    "                if t1 != t2 and t1 in sufficientNodes and t2 in sufficientNodes:\n",
    "                    label = '_'.join(sorted([t1, t2]))\n",
    "                    linkCounter.setdefault(label, 0)\n",
    "                    linkCounter[label] += 1\n",
    "                    polarityOfLinks.setdefault(label, [])\n",
    "                    polarityOfLinks[label].append(polarityScore)\n",
    "        print('\\tgot linked tokens')\n",
    "        node2id = dict()\n",
    "        for i, node in enumerate(sufficientNodes):\n",
    "            networkJson['nodes'].append({\n",
    "                'id' : node,\n",
    "                'occurrences' : keywordCounter[node],\n",
    "                'nodePolarity' : sum(polarityOfNode[node])/len(polarityOfNode[node])\n",
    "            })\n",
    "            node2id[node] = i\n",
    "\n",
    "        for label in linkCounter:\n",
    "            source, target = label.split('_')\n",
    "            networkJson['links'].append({\n",
    "                'source' : node2id[source],\n",
    "                'target' : node2id[target],\n",
    "                'value' : linkCounter[label],\n",
    "                'polarityScore' : sum(polarityOfLinks[label])/len(polarityOfLinks[label])\n",
    "            })\n",
    "\n",
    "        jsonDict[cat] = networkJson\n",
    "        topicDict[topicID][cat] = networkJson\n",
    "        print(f'successfully parsed {len(networkJson[\"nodes\"])} nodes and {len(networkJson[\"links\"])} links')\n",
    "\n",
    "        # with open(f'visualizations/adjmatrix/topic_data/{cat}network{n}_topic{topicID}_adjmatrix.json', 'w', encoding ='ascii') as json_file:\n",
    "        #     json.dump(networkJson, json_file, ensure_ascii = True)\n",
    "        # print('\\twrote json for topic',topicID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8c2bc0-f42f-4829-a8f9-051de2ff6974",
   "metadata": {},
   "source": [
    "## T-test on sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7df6dc-6886-40da-bf8c-a5736e114f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9015d037-41b7-42f4-8777-ff7d7228bb29",
   "metadata": {},
   "source": [
    "## Louvain clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "99c59cd8-7550-4d33-b9d9-ea0c9b19ada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in ['western','russian']:\n",
    "    smallJson = jsonDict[cat]\n",
    "\n",
    "    numNodes = len(smallJson['nodes'])\n",
    "    adjArray = np.zeros((numNodes, numNodes), dtype=int)\n",
    "    for l in smallJson['links']:\n",
    "        i = l['source']\n",
    "        j = l['target']\n",
    "        adjArray[i,j] = l['value']\n",
    "        adjArray[j,i] = l['value']\n",
    "\n",
    "    louvain = Louvain()\n",
    "    labels = louvain.fit_transform(adjArray)\n",
    "    len(set(labels))\n",
    "\n",
    "    for i, n in enumerate(smallJson['nodes']):\n",
    "        n['group'] = int(louvain.labels_[i])\n",
    "\n",
    "    n = 5\n",
    "    with open(f'visualizations/adjmatrix/{cat}network{n}_topic{topicID}_adjmatrix.json', 'w', encoding ='ascii') as json_file:\n",
    "        json.dump(smallJson, json_file, ensure_ascii = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5445c52e-e522-4724-aabd-2f9a549077f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b97d942-0805-4079-9f66-e4f0f64fc250",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
