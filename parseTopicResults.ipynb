{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31f15ac4-c675-44e1-8879-e1518ce81fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8430fadf-2d5d-4ce4-a368-1cfc8f53a19e",
   "metadata": {},
   "source": [
    "### Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96142ef2-ab86-4c9d-becb-d366b411c334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56609\n",
      "53349\n",
      "CPU times: user 7.35 s, sys: 123 ms, total: 7.47 s\n",
      "Wall time: 7.62 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>newsOutlet</th>\n",
       "      <th>category</th>\n",
       "      <th>dateSeen</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>language</th>\n",
       "      <th>sourceCountry</th>\n",
       "      <th>sentText</th>\n",
       "      <th>sentIndexInText</th>\n",
       "      <th>sentTopicID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>apnews.com</td>\n",
       "      <td>western</td>\n",
       "      <td>2021-10-12 07:00:00</td>\n",
       "      <td>https://apnews.com/article/technology-business...</td>\n",
       "      <td>EU , Ukraine to discuss military training and ...</td>\n",
       "      <td>English</td>\n",
       "      <td>United States</td>\n",
       "      <td>BRUSSELS (AP) — The European Union is consider...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>apnews.com</td>\n",
       "      <td>western</td>\n",
       "      <td>2021-10-12 07:00:00</td>\n",
       "      <td>https://apnews.com/article/technology-business...</td>\n",
       "      <td>EU , Ukraine to discuss military training and ...</td>\n",
       "      <td>English</td>\n",
       "      <td>United States</td>\n",
       "      <td>Acting on a request from Ukraine for help with...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>apnews.com</td>\n",
       "      <td>western</td>\n",
       "      <td>2021-10-12 07:00:00</td>\n",
       "      <td>https://apnews.com/article/technology-business...</td>\n",
       "      <td>EU , Ukraine to discuss military training and ...</td>\n",
       "      <td>English</td>\n",
       "      <td>United States</td>\n",
       "      <td>The topic will be discussed during a summit Tu...</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>apnews.com</td>\n",
       "      <td>western</td>\n",
       "      <td>2021-10-12 07:00:00</td>\n",
       "      <td>https://apnews.com/article/technology-business...</td>\n",
       "      <td>EU , Ukraine to discuss military training and ...</td>\n",
       "      <td>English</td>\n",
       "      <td>United States</td>\n",
       "      <td>The results of the mission have yet to be anal...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>apnews.com</td>\n",
       "      <td>western</td>\n",
       "      <td>2021-10-12 07:00:00</td>\n",
       "      <td>https://apnews.com/article/technology-business...</td>\n",
       "      <td>EU , Ukraine to discuss military training and ...</td>\n",
       "      <td>English</td>\n",
       "      <td>United States</td>\n",
       "      <td>One official said the EU’s political and secur...</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  newsOutlet category             dateSeen  \\\n",
       "0      0  apnews.com  western  2021-10-12 07:00:00   \n",
       "1      0  apnews.com  western  2021-10-12 07:00:00   \n",
       "2      0  apnews.com  western  2021-10-12 07:00:00   \n",
       "3      0  apnews.com  western  2021-10-12 07:00:00   \n",
       "4      0  apnews.com  western  2021-10-12 07:00:00   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://apnews.com/article/technology-business...   \n",
       "1  https://apnews.com/article/technology-business...   \n",
       "2  https://apnews.com/article/technology-business...   \n",
       "3  https://apnews.com/article/technology-business...   \n",
       "4  https://apnews.com/article/technology-business...   \n",
       "\n",
       "                                               title language  sourceCountry  \\\n",
       "0  EU , Ukraine to discuss military training and ...  English  United States   \n",
       "1  EU , Ukraine to discuss military training and ...  English  United States   \n",
       "2  EU , Ukraine to discuss military training and ...  English  United States   \n",
       "3  EU , Ukraine to discuss military training and ...  English  United States   \n",
       "4  EU , Ukraine to discuss military training and ...  English  United States   \n",
       "\n",
       "                                            sentText  sentIndexInText  \\\n",
       "0  BRUSSELS (AP) — The European Union is consider...                0   \n",
       "1  Acting on a request from Ukraine for help with...                1   \n",
       "2  The topic will be discussed during a summit Tu...                2   \n",
       "3  The results of the mission have yet to be anal...                3   \n",
       "4  One official said the EU’s political and secur...                4   \n",
       "\n",
       "   sentTopicID  \n",
       "0           -1  \n",
       "1           -1  \n",
       "2           -1  \n",
       "3            5  \n",
       "4           -1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sdf = pd.read_excel('data/topic_model_data/fullSentDataFrame_8-3.xlsx',index_col=0)\n",
    "print(len(sdf))\n",
    "sdf = sdf.drop_duplicates(['sentText'])\n",
    "print(len(sdf))\n",
    "sdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7edc9bb0-a2b3-4260-b618-5f6da14d9c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/paigelee/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/paigelee/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.vader import SentiText\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6e76dcb2-dafa-4001-9d17-b122caa91452",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/sentiment/vader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lexicon_file)\u001b[0m\n\u001b[1;32m    339\u001b[0m     ):\n\u001b[1;32m    340\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexicon_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlexicon_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexicon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_lex_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVaderConstants\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/sentiment/vader.py\u001b[0m in \u001b[0;36mmake_lex_dict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mlex_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexicon_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeasure\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m             \u001b[0mlex_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeasure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlex_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# add sentiment row to DF\n",
    "sentiRecs = []\n",
    "records = sdf.to_dict('records')\n",
    "for rec in records:\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    ss = sid.polarity_scores(rec['sentText'])\n",
    "    artSenti = dict()\n",
    "    for k in ['pos','neu','neg']:\n",
    "        artSenti[k] = ss[k]\n",
    "    sentiRecs.append(artSenti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d6ecf9b7-c32d-4e86-8a31-ed9ab4134e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>newsOutlet</th>\n",
       "      <th>category</th>\n",
       "      <th>dateSeen</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>language</th>\n",
       "      <th>sourceCountry</th>\n",
       "      <th>sentText</th>\n",
       "      <th>sentIndexInText</th>\n",
       "      <th>sentTopicID</th>\n",
       "      <th>pos</th>\n",
       "      <th>neu</th>\n",
       "      <th>neg</th>\n",
       "      <th>sentiVal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46458</th>\n",
       "      <td>2342.0</td>\n",
       "      <td>tass.com</td>\n",
       "      <td>russian</td>\n",
       "      <td>2022-01-01 08:00:00</td>\n",
       "      <td>https://tass.com/world/1383393</td>\n",
       "      <td>Israeli embassy in Kiev condemns nationalists ...</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Any attempt to glorify those who supported Naz...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25270</th>\n",
       "      <td>1010.0</td>\n",
       "      <td>sputniknews.com</td>\n",
       "      <td>russian</td>\n",
       "      <td>2022-01-03 08:00:00</td>\n",
       "      <td>https://sputniknews.com/20220103/ramped-up-us-...</td>\n",
       "      <td>Ramped Up US Sanctions Sending Cuba Closer to ...</td>\n",
       "      <td>English</td>\n",
       "      <td>Russia</td>\n",
       "      <td>The People’s Republic of China (PRC) is alread...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25388</th>\n",
       "      <td>1013.0</td>\n",
       "      <td>sputniknews.com</td>\n",
       "      <td>russian</td>\n",
       "      <td>2021-11-22 08:00:00</td>\n",
       "      <td>https://sputniknews.com/20211122/scholars-us--...</td>\n",
       "      <td>Scholars : US &amp; EU Using  Ukraine Invasion  St...</td>\n",
       "      <td>English</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Third, the positions of the Democratic neo-lib...</td>\n",
       "      <td>37.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>36.0</td>\n",
       "      <td>apnews.com</td>\n",
       "      <td>western</td>\n",
       "      <td>2021-10-27 07:00:00</td>\n",
       "      <td>https://apnews.com/9a111a737a6dd320f2812ccbf35...</td>\n",
       "      <td>WADA says Ukraine routinely violated rules for...</td>\n",
       "      <td>English</td>\n",
       "      <td>United States</td>\n",
       "      <td>The investigation also looked into allegations...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        index       newsOutlet category            dateSeen  \\\n",
       "46458  2342.0         tass.com  russian 2022-01-01 08:00:00   \n",
       "25270  1010.0  sputniknews.com  russian 2022-01-03 08:00:00   \n",
       "25388  1013.0  sputniknews.com  russian 2021-11-22 08:00:00   \n",
       "784      36.0       apnews.com  western 2021-10-27 07:00:00   \n",
       "\n",
       "                                                     url  \\\n",
       "46458                     https://tass.com/world/1383393   \n",
       "25270  https://sputniknews.com/20220103/ramped-up-us-...   \n",
       "25388  https://sputniknews.com/20211122/scholars-us--...   \n",
       "784    https://apnews.com/9a111a737a6dd320f2812ccbf35...   \n",
       "\n",
       "                                                   title language  \\\n",
       "46458  Israeli embassy in Kiev condemns nationalists ...  English   \n",
       "25270  Ramped Up US Sanctions Sending Cuba Closer to ...  English   \n",
       "25388  Scholars : US & EU Using  Ukraine Invasion  St...  English   \n",
       "784    WADA says Ukraine routinely violated rules for...  English   \n",
       "\n",
       "       sourceCountry                                           sentText  \\\n",
       "46458            NaN  Any attempt to glorify those who supported Naz...   \n",
       "25270         Russia  The People’s Republic of China (PRC) is alread...   \n",
       "25388         Russia  Third, the positions of the Democratic neo-lib...   \n",
       "784    United States  The investigation also looked into allegations...   \n",
       "\n",
       "       sentIndexInText  sentTopicID    pos    neu   neg  sentiVal  \n",
       "46458              2.0         14.0    NaN    NaN   NaN       NaN  \n",
       "25270             23.0         -1.0  0.130  0.870  0.00     0.130  \n",
       "25388             37.0         -1.0  0.095  0.905  0.00     0.095  \n",
       "784               10.0         -1.0  0.068  0.802  0.13    -0.062  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentOnlyDf = pd.DataFrame.from_records(sentiRecs)\n",
    "df = pd.concat([sdf, sentOnlyDf],axis=1)\n",
    "df['dateSeen'] = df['dateSeen'].apply(lambda d: pd.to_datetime(d))\n",
    "df['sentiVal'] = df['pos']-df['neg']\n",
    "df.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c1e455-4618-4e34-880a-cce862b937b9",
   "metadata": {},
   "source": [
    "### Write JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "87ca0273-4cd3-4f7e-b329-ca5447db4702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsonDf = df.copy(deep=True)\n",
    "# jsonDf['dateSeen'] = jsonDf['dateSeen'].apply(lambda d : d.timestamp())\n",
    "# del jsonDf['month']\n",
    "# del jsonDf['sourceCountry']\n",
    "\n",
    "# with open('visualizations/histograms/sentData.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(jsonDf.to_dict('records'), f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae57e58b-8b1a-4cdb-bd5f-a50755ae7200",
   "metadata": {},
   "source": [
    "## Reading scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c1ff66-04cb-468f-9848-1dd633934fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a126848-8a3f-4d11-aecf-6b19dae3dabd",
   "metadata": {},
   "source": [
    "## Create Topic Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "afdd8727-c9bb-4a93-8c2b-50c49d9b248f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TopicID</th>\n",
       "      <th>Topic Keywords</th>\n",
       "      <th>Topic Summary</th>\n",
       "      <th>Western Count</th>\n",
       "      <th>Russian Count</th>\n",
       "      <th>MoscowTimes Count</th>\n",
       "      <th>Total Count</th>\n",
       "      <th>Freq in Western</th>\n",
       "      <th>Freq in Russian</th>\n",
       "      <th>Freq in MoscowTimes</th>\n",
       "      <th>Total Freq</th>\n",
       "      <th>Western Senti Avg</th>\n",
       "      <th>Russian Senti Avg</th>\n",
       "      <th>MoscowTimes Senti Avg</th>\n",
       "      <th>Average Senti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>777</td>\n",
       "      <td>1318</td>\n",
       "      <td>180</td>\n",
       "      <td>2275</td>\n",
       "      <td>0.035079</td>\n",
       "      <td>0.047125</td>\n",
       "      <td>0.055710</td>\n",
       "      <td>0.041719</td>\n",
       "      <td>-0.019656</td>\n",
       "      <td>-0.015580</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.017958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>611</td>\n",
       "      <td>585</td>\n",
       "      <td>75</td>\n",
       "      <td>1271</td>\n",
       "      <td>0.027585</td>\n",
       "      <td>0.020917</td>\n",
       "      <td>0.023213</td>\n",
       "      <td>0.023308</td>\n",
       "      <td>-0.015396</td>\n",
       "      <td>0.005340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.011441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1957</td>\n",
       "      <td>3444</td>\n",
       "      <td>408</td>\n",
       "      <td>5809</td>\n",
       "      <td>0.088352</td>\n",
       "      <td>0.123141</td>\n",
       "      <td>0.126277</td>\n",
       "      <td>0.106527</td>\n",
       "      <td>-0.015825</td>\n",
       "      <td>-0.016890</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.016224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>386</td>\n",
       "      <td>298</td>\n",
       "      <td>47</td>\n",
       "      <td>731</td>\n",
       "      <td>0.017427</td>\n",
       "      <td>0.010655</td>\n",
       "      <td>0.014547</td>\n",
       "      <td>0.013405</td>\n",
       "      <td>-0.000355</td>\n",
       "      <td>0.011044</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>267</td>\n",
       "      <td>410</td>\n",
       "      <td>75</td>\n",
       "      <td>752</td>\n",
       "      <td>0.012054</td>\n",
       "      <td>0.014660</td>\n",
       "      <td>0.023213</td>\n",
       "      <td>0.013790</td>\n",
       "      <td>-0.041041</td>\n",
       "      <td>-0.015712</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.032964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>836</td>\n",
       "      <td>797</td>\n",
       "      <td>73</td>\n",
       "      <td>1706</td>\n",
       "      <td>0.037743</td>\n",
       "      <td>0.028497</td>\n",
       "      <td>0.022594</td>\n",
       "      <td>0.031285</td>\n",
       "      <td>-0.004642</td>\n",
       "      <td>-0.012403</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.006418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    TopicID Topic Keywords Topic Summary  Western Count  Russian Count  \\\n",
       "6         5                                         777           1318   \n",
       "5         4                                         611            585   \n",
       "14       13                                        1957           3444   \n",
       "9         8                                         386            298   \n",
       "2         1                                         267            410   \n",
       "1         0                                         836            797   \n",
       "\n",
       "    MoscowTimes Count  Total Count  Freq in Western  Freq in Russian  \\\n",
       "6                 180         2275         0.035079         0.047125   \n",
       "5                  75         1271         0.027585         0.020917   \n",
       "14                408         5809         0.088352         0.123141   \n",
       "9                  47          731         0.017427         0.010655   \n",
       "2                  75          752         0.012054         0.014660   \n",
       "1                  73         1706         0.037743         0.028497   \n",
       "\n",
       "    Freq in MoscowTimes  Total Freq  Western Senti Avg  Russian Senti Avg  \\\n",
       "6              0.055710    0.041719          -0.019656          -0.015580   \n",
       "5              0.023213    0.023308          -0.015396           0.005340   \n",
       "14             0.126277    0.106527          -0.015825          -0.016890   \n",
       "9              0.014547    0.013405          -0.000355           0.011044   \n",
       "2              0.023213    0.013790          -0.041041          -0.015712   \n",
       "1              0.022594    0.031285          -0.004642          -0.012403   \n",
       "\n",
       "    MoscowTimes Senti Avg  Average Senti  \n",
       "6                     NaN      -0.017958  \n",
       "5                     NaN      -0.011441  \n",
       "14                    NaN      -0.016224  \n",
       "9                     NaN       0.001820  \n",
       "2                     NaN      -0.032964  \n",
       "1                     NaN      -0.006418  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TopicID\tTopic Keywords\tTopic Summary\tWestern Count\tWestern Freq\tRussian Count\tRussian Freq\tMoscowTimes Count\tMoscowTimes Freq\tTotal Count\n",
    "numTopics = 15\n",
    "topicIDs = range(-1,numTopics)\n",
    "topicStatData = []\n",
    "for topicID in topicIDs:\n",
    "    topicRec = dict()\n",
    "    topicRec['TopicID'] = topicID\n",
    "    topicRec['Topic Keywords'] = ''\n",
    "    topicRec['Topic Summary'] = ''\n",
    "    \n",
    "    wdf = df[df['category']=='western']\n",
    "    rdf = df[df['category']=='russian']\n",
    "    mtdf = df[df['category']=='moscowtimes']\n",
    "    \n",
    "    thiswdf = wdf[wdf['sentTopicID']==topicID]\n",
    "    thisrdf = rdf[rdf['sentTopicID']==topicID]\n",
    "    thismtdf = mtdf[mtdf['sentTopicID']==topicID]\n",
    "  \n",
    "    topicRec['Western Count'] = len(wdf[wdf['sentTopicID']==topicID])\n",
    "    topicRec['Russian Count'] = len(rdf[rdf['sentTopicID']==topicID])\n",
    "    topicRec['MoscowTimes Count'] = len(mtdf[mtdf['sentTopicID']==topicID])\n",
    "    topicRec['Total Count'] = len(df[df['sentTopicID']==topicID])\n",
    "    \n",
    "    topicRec['Freq in Western'] = topicRec['Western Count'] / len(wdf)\n",
    "    topicRec['Freq in Russian'] = topicRec['Russian Count'] / len(rdf)\n",
    "    topicRec['Freq in MoscowTimes'] = topicRec['MoscowTimes Count'] / len(mtdf)\n",
    "    topicRec['Total Freq'] = len(df[df['sentTopicID']==topicID])/len(df)\n",
    "    \n",
    "    topicRec['Western Senti Avg'] = thiswdf['pos'].mean()- thiswdf['neg'].mean()\n",
    "    topicRec['Russian Senti Avg'] = thisrdf['pos'].mean() - thisrdf['neg'].mean()\n",
    "    topicRec['MoscowTimes Senti Avg'] = thismtdf['pos'].mean()- thismtdf['neg'].mean()\n",
    "    topicRec['Average Senti'] = df[df['sentTopicID']==topicID]['pos'].mean() - df[df['sentTopicID']==topicID]['neg'].mean()\n",
    "    \n",
    "    topicStatData.append(topicRec)\n",
    "    \n",
    "topicStats = pd.DataFrame.from_records(topicStatData)\n",
    "topicStats.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6b7952e9-54a6-45cd-9b0a-4fccd9e4c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "topicStats.to_excel('data/topic_model_data/topics_and_sentiment_8-3.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ef492a-8d72-4c81-935f-45d05a88717c",
   "metadata": {},
   "source": [
    "### Topic Timeline Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e2d30fda-b7d1-4287-bb8f-82e30c66a988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6a7a7029-4d6f-484a-889a-202bfa8d4469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.7 s, sys: 244 ms, total: 5.95 s\n",
      "Wall time: 7.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "totalNumTopics= 15\n",
    "timeTuples = [\n",
    "    (\"2021-10-01\", \"2021-10-15\"), \n",
    "    (\"2021-10-16\", \"2021-10-31\"), \n",
    "    (\"2021-11-01\", \"2021-11-15\"), \n",
    "    (\"2021-11-16\", \"2021-11-30\"), \n",
    "    (\"2021-12-01\", \"2021-12-15\"), \n",
    "    (\"2021-12-16\", \"2021-12-31\"),    \n",
    "    (\"2022-01-01\", \"2022-01-15\"),    \n",
    "    (\"2022-01-16\", \"2022-01-31\"),\n",
    "    (\"2022-02-01\", \"2022-02-15\"),\n",
    "    (\"2022-02-16\", \"2022-02-28\"),]\n",
    "\n",
    "records = df.to_dict('records')\n",
    "\n",
    "biweeklyDict = dict()\n",
    "recCatCounts = dict()\n",
    "for rec in records:\n",
    "    if rec['category'] in ['western','russian']:\n",
    "        # add rec to timerange dict\n",
    "        for i, (startStr, endStr) in enumerate(timeTuples):\n",
    "            startDate = datetime.strptime(startStr, '%Y-%m-%d')\n",
    "            endDate = datetime.strptime(endStr, '%Y-%m-%d')\n",
    "            if rec['dateSeen'] >= startDate and rec['dateSeen'] <= endDate:\n",
    "                biweeklyDict.setdefault(i, dict())\n",
    "                for topicID in range(-1,totalNumTopics):\n",
    "                    biweeklyDict[i].setdefault(topicID, [])\n",
    "                biweeklyDict[i][rec['sentTopicID']].append(rec)\n",
    "                recCatCounts.setdefault(i, {'western' : 0,'russian' : 0})\n",
    "                recCatCounts[i][rec['category']]+= 1\n",
    "jsonForTimeline = []\n",
    "for i in biweeklyDict:\n",
    "    row = dict()\n",
    "    row['date'] = timeTuples[i][0].replace('-','')\n",
    "    row[f'western_ct_period_{i}'] = recCatCounts[i]['western']\n",
    "    row[f'russian_ct_period_{i}'] = recCatCounts[i]['russian']\n",
    "    for topicID in range(-1,totalNumTopics):\n",
    "        row['russian_freq_topic_'+str(topicID)] = len([r for r in biweeklyDict[i][topicID] if r['category']=='russian'])/recCatCounts[i]['russian']\n",
    "        row['total_freq_topic_'+str(topicID)] = len(biweeklyDict[i][topicID])/(recCatCounts[i]['russian']+recCatCounts[i]['western'])\n",
    "        row['western_freq_topic_'+str(topicID)] = len([r for r in biweeklyDict[i][topicID] if r['category']=='western'])/recCatCounts[i]['western']\n",
    "        row['russian_topic_'+str(topicID)] = len([r for r in biweeklyDict[i][topicID] if r['category']=='russian'])\n",
    "        row['western_topic_'+str(topicID)] = len([r for r in biweeklyDict[i][topicID] if r['category']=='western'])\n",
    "        row['total_topic_'+str(topicID)] = len(biweeklyDict[i][topicID])\n",
    "    jsonForTimeline.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dbecbb89-8387-4450-adee-fd66b7355bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('visualizations/timeline/freq_data_8-3.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(jsonForTimeline, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f876fc6a-f0c3-41d7-9e46-e3f263bb8420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bab81e4-f1b4-459f-8827-7e835e76aaf6",
   "metadata": {},
   "source": [
    "## Co-occurrence analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26518663-acce-4d5e-ac0e-28742accfea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76db2db3-3b9d-4844-8678-9d00a45bdd74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4781bcb56c894dbe87e2d0e619d97b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.2.2.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-04 17:05:33 INFO: Downloading default packages for language: en (English)...\n",
      "2022-08-04 17:05:34 INFO: File exists: /Users/paigelee/stanza_resources/en/default.zip.\n",
      "2022-08-04 17:05:37 INFO: Finished downloading models and saved to /Users/paigelee/stanza_resources.\n",
      "2022-08-04 17:05:37 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2022-08-04 17:05:37 INFO: Use device: cpu\n",
      "2022-08-04 17:05:37 INFO: Loading: tokenize\n",
      "2022-08-04 17:05:37 INFO: Loading: pos\n",
      "2022-08-04 17:05:38 INFO: Loading: lemma\n",
      "2022-08-04 17:05:38 INFO: Loading: depparse\n",
      "2022-08-04 17:05:38 INFO: Loading: sentiment\n",
      "2022-08-04 17:05:38 INFO: Loading: ner\n",
      "2022-08-04 17:05:39 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8baee06e-cd7b-4b58-b371-dc89f65fe269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "engStopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5ec5c116-925e-4639-9cbc-e2417e256aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonDict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "60827ca2-26be-4ab9-9bab-747154990d37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/12351 recs parsed by NLP\n",
      "1000/12351 recs parsed by NLP\n",
      "2000/12351 recs parsed by NLP\n",
      "3000/12351 recs parsed by NLP\n",
      "4000/12351 recs parsed by NLP\n",
      "5000/12351 recs parsed by NLP\n",
      "6000/12351 recs parsed by NLP\n",
      "7000/12351 recs parsed by NLP\n",
      "8000/12351 recs parsed by NLP\n",
      "9000/12351 recs parsed by NLP\n",
      "10000/12351 recs parsed by NLP\n",
      "11000/12351 recs parsed by NLP\n",
      "12000/12351 recs parsed by NLP\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 374 nodes and 20577 links\n",
      "\twrote json\n",
      "0/16394 recs parsed by NLP\n",
      "1000/16394 recs parsed by NLP\n",
      "2000/16394 recs parsed by NLP\n",
      "3000/16394 recs parsed by NLP\n",
      "4000/16394 recs parsed by NLP\n",
      "5000/16394 recs parsed by NLP\n",
      "6000/16394 recs parsed by NLP\n",
      "7000/16394 recs parsed by NLP\n",
      "8000/16394 recs parsed by NLP\n",
      "9000/16394 recs parsed by NLP\n",
      "10000/16394 recs parsed by NLP\n",
      "11000/16394 recs parsed by NLP\n",
      "12000/16394 recs parsed by NLP\n",
      "13000/16394 recs parsed by NLP\n",
      "14000/16394 recs parsed by NLP\n",
      "15000/16394 recs parsed by NLP\n",
      "16000/16394 recs parsed by NLP\n",
      "\tdone nlping docs\n",
      "\tfound sufficient recs\n",
      "\tgot linked tokens\n",
      "successfully parsed 519 nodes and 35346 links\n",
      "\twrote json\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# get cooccurrence connections\n",
    "thresh = 100\n",
    "n = 5\n",
    "\n",
    "totalWordCt = {\n",
    "    'western' : 0,\n",
    "    'russian' : 0\n",
    "}\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "allSentRecords = sdf.to_dict('records')\n",
    "\n",
    "\n",
    "for cat in ['western','russian']:\n",
    "    networkJson = dict()\n",
    "    networkJson['nodes'] = []\n",
    "    networkJson['links'] = []\n",
    "\n",
    "    curatedNodes = set()\n",
    "    linkCounter = dict()\n",
    "    keywordCounter = dict()\n",
    "    polarityOfNode = dict()\n",
    "\n",
    "    windowList = []\n",
    "    records = []\n",
    "    for d in allSentRecords:\n",
    "        if d['category'] == cat and d['sentTopicID'] != -1:\n",
    "            records.append(d)\n",
    "            \n",
    "    random.shuffle(records)\n",
    "    for i, rec in enumerate(records):\n",
    "        if i % 1000 == 0:\n",
    "            print(f'{i}/{len(records)} recs parsed by NLP')\n",
    "        docText = rec['sentText']\n",
    "        nlpdoc = nlp(docText)\n",
    "\n",
    "        ss = sid.polarity_scores(docText)\n",
    "        polarityScore = ss['pos'] - ss['neg']\n",
    "        \n",
    "        wordList = [w for w in nlpdoc.iter_words() if w.text.isalpha() and w.text.lower() not in engStopwords]\n",
    "        for windowIdx in range(len(wordList) // n + len(wordList) % n):\n",
    "            startIdx = windowIdx\n",
    "            endIdx = windowIdx+random.choice(list(range(1,n+1)))\n",
    "            wordWindow = wordList[startIdx:endIdx]\n",
    "            tokens = []\n",
    "            for wordToken in wordWindow:\n",
    "                formattedToken = wordToken.lemma.lower()\n",
    "                if formattedToken in engStopwords:\n",
    "                    continue\n",
    "                tokens.append(formattedToken)\n",
    "            windowList.append((tokens, polarityScore))\n",
    "        for t1 in [w.lemma.lower() for w in wordList]:\n",
    "            keywordCounter.setdefault(t1, 0)\n",
    "            keywordCounter[t1] += 1\n",
    "            polarityOfNode.setdefault(t1, [])\n",
    "            polarityOfNode[t1].append(polarityScore)\n",
    "            totalWordCt[cat] += 1\n",
    "        if totalWordCt[cat] >= 300000:\n",
    "            print(f'stopping the {cat} count at {totalWordCt[cat]}words')\n",
    "            break\n",
    "    print('\\tdone nlping docs')\n",
    "    sufficientNodes = set()\n",
    "    for key in keywordCounter:\n",
    "        if keywordCounter[key] >= thresh:\n",
    "            sufficientNodes.add(key)\n",
    "    print('\\tfound sufficient recs')\n",
    "    polarityOfLinks= dict()\n",
    "    \n",
    "    for tokens, polarityScore in windowList:\n",
    "        s = set(tokens)\n",
    "        subsets = [tuple(i) for i in itertools.combinations(s, 2)]\n",
    "        for t1, t2 in subsets:\n",
    "            if t1 != t2 and t1 in sufficientNodes and t2 in sufficientNodes:\n",
    "                label = '_'.join(sorted([t1, t2]))\n",
    "                linkCounter.setdefault(label, 0)\n",
    "                linkCounter[label] += 1\n",
    "                polarityOfLinks.setdefault(label, [])\n",
    "                polarityOfLinks[label].append(polarityScore)\n",
    "    print('\\tgot linked tokens')\n",
    "    node2id = dict()\n",
    "    for i, node in enumerate(sufficientNodes):\n",
    "        networkJson['nodes'].append({\n",
    "            'id' : node,\n",
    "            'occurrences' : keywordCounter[node],\n",
    "            'nodePolarity' : sum(polarityOfNode[node])/len(polarityOfNode[node])\n",
    "        })\n",
    "        node2id[node] = i\n",
    "\n",
    "    for label in linkCounter:\n",
    "        source, target = label.split('_')\n",
    "        networkJson['links'].append({\n",
    "            'source' : node2id[source],\n",
    "            'target' : node2id[target],\n",
    "            'value' : linkCounter[label],\n",
    "            'polarityScore' : sum(polarityOfLinks[label])/len(polarityOfLinks[label])\n",
    "        })\n",
    "    \n",
    "    jsonDict[cat] = networkJson\n",
    "    print(f'successfully parsed {len(networkJson[\"nodes\"])} nodes and {len(networkJson[\"links\"])} links')\n",
    "    \n",
    "    # with open(f'visualizations/adjmatrix/{cat}network{n}foradjmatrix_8-5_.json', 'w', encoding ='ascii') as json_file:\n",
    "    #     json.dump(networkJson, json_file, ensure_ascii = True)\n",
    "    # print('\\twrote json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "03033ace-0931-431f-ba36-a68fd3bd45ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\twrote json\n"
     ]
    }
   ],
   "source": [
    "with open(f'visualizations/ngramsNetwork/{cat}ngramNet{n}_8-5_w_nodeandlinkpolarity.json', 'w', encoding ='ascii') as json_file:\n",
    "    json.dump(networkJson, json_file, ensure_ascii = True)\n",
    "print('\\twrote json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4c923d8d-7492-4e51-801e-212fdbbfe0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'western': 153791, 'russian': 0}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalWordCt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "56b565e9-c05c-4b82-9e3c-e1a9df20f88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'visualizations/ngramsNetwork/{cat}ngramNet{n}_8-4.json', 'w', encoding ='ascii') as json_file:\n",
    "#     json.dump(networkJson, json_file, ensure_ascii = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f959c98e-ea37-405d-94aa-0cd3ede8f2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "newJsonDict = jsonDict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b3d00d8-d163-4682-a9fe-b1282a74b32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "western has 12351 recs and 367218 words\n",
      "russian has 16394 recs and 505646 words\n"
     ]
    }
   ],
   "source": [
    "for cat in jsonDict:\n",
    "    ct = 0\n",
    "    words = 0\n",
    "    for d in allSentRecords:\n",
    "        if d['category'] == cat and d['sentTopicID'] != -1:\n",
    "            ct += 1\n",
    "            words += len(word_tokenize(d['sentText']))\n",
    "    print(cat,'has',ct,'recs and',words,'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f51984b-9f67-4a5a-8bf2-3c7fb1d3bac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3769640921741308"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ID2node = {\n",
    "    'western' : dict(),\n",
    "    'russian' : dict()\n",
    "}\n",
    "for cat in jsonDict:\n",
    "    for i, n in enumerate(jsonDict[cat]['nodes']):\n",
    "        ID2node[cat][i] = n['id']\n",
    "node2ID = {\n",
    "    'russian' : node2id,\n",
    "    'western' : dict()\n",
    "}\n",
    "for i, node in enumerate(jsonDict['western']['nodes']):\n",
    "    node2ID['western'][node['id']]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14878ab9-e37a-4c15-a908-b4ca02d15daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "western {'source': 1456, 'target': 2503, 'value': 50, 'polarityScore': -0.1241} ukraine tension\n",
      "western {'source': 2180, 'target': 2503, 'value': 65, 'polarityScore': -0.1758615384615385} ukraine plan\n",
      "western {'source': 3189, 'target': 2503, 'value': 19, 'polarityScore': 0.048736842105263155} ukraine situation\n",
      "russian {'source': 1593, 'target': 2709, 'value': 107, 'polarityScore': -0.08937383177570091} ukraine tension\n",
      "russian {'source': 2360, 'target': 2709, 'value': 86, 'polarityScore': -0.052697674418604686} ukraine plan\n",
      "russian {'source': 3468, 'target': 2709, 'value': 122, 'polarityScore': -0.0074590163934426254} ukraine situation\n"
     ]
    }
   ],
   "source": [
    "# how often plan & ukraine or situation & ukraine\n",
    "for cat in jsonDict:\n",
    "    for link in jsonDict[cat]['links']:\n",
    "        for word in ['plan','situation','tension']:\n",
    "            if set([link['source'],link['target']]) == set([node2ID[cat]['ukraine'],node2ID[cat][word]]):\n",
    "                print(cat, link, 'ukraine',word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a9b19d4-2cf6-4185-8425-7d54b8672d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "western {'source': 2686, 'target': 2730, 'value': 1, 'polarityScore': 0.688} 0.688\n",
      "russian {'source': 3564, 'target': 2194, 'value': 1, 'polarityScore': 0.515} 0.515\n"
     ]
    }
   ],
   "source": [
    "# highest polarity score\n",
    "for cat in jsonDict:\n",
    "    maxPol = 0\n",
    "    maxLink = ''\n",
    "    for link in jsonDict[cat]['links']:\n",
    "        if link['polarityScore'] > maxPol:\n",
    "            maxPol = link['polarityScore']\n",
    "            maxLink = link\n",
    "    print(cat, maxLink,maxPol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "41686219-444c-4a7e-b025-5e1ab5b5f76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('good', 'like')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ID2node['western'][2686],ID2node['western'][2730]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "575583e0-7ac9-46b0-807d-ab0e42639e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('peace', 'prominent')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ID2node['russian'][3564],ID2node['western'][2194]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c0ee3a6-966b-405e-8b8a-1cdccc5b49d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed125dfc-47ff-4b43-a05e-0f9388774a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f70c9127-0eb8-4029-bd9d-a44fec5e37db",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5ee8cb8-7047-4b2e-a9c6-f5c79ab4de31",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/topic model data/topicArticleFrame7-31.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jj/_szc94p56q91q_7c209d1d9w0000gn/T/ipykernel_3197/1802861011.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/topic model data/topicArticleFrame7-31.xlsx'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1374\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1377\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m                 )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1248\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1251\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m     ) as handle:\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/topic model data/topicArticleFrame7-31.xlsx'"
     ]
    }
   ],
   "source": [
    "# df = pd.read_excel('data/topic model data/topicArticleFrame7-31.xlsx',index_col=0)\n",
    "# df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d00b41e-4b5b-4b0f-a026-4c2fe6d76c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TopicID</th>\n",
       "      <th>Topic Keywords</th>\n",
       "      <th>Topic Summary</th>\n",
       "      <th>Western Count</th>\n",
       "      <th>Freq in Western</th>\n",
       "      <th>Russian Count</th>\n",
       "      <th>Freq in Russian</th>\n",
       "      <th>MoscowTimes Count</th>\n",
       "      <th>Freq in MoscowTimes</th>\n",
       "      <th>Total Count</th>\n",
       "      <th>Total Freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>469</td>\n",
       "      <td>0.018757</td>\n",
       "      <td>674</td>\n",
       "      <td>0.022437</td>\n",
       "      <td>22</td>\n",
       "      <td>0.006681</td>\n",
       "      <td>1165</td>\n",
       "      <td>0.019971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>792</td>\n",
       "      <td>0.031675</td>\n",
       "      <td>1289</td>\n",
       "      <td>0.042911</td>\n",
       "      <td>148</td>\n",
       "      <td>0.044944</td>\n",
       "      <td>2229</td>\n",
       "      <td>0.038210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>138</td>\n",
       "      <td>0.005519</td>\n",
       "      <td>629</td>\n",
       "      <td>0.020939</td>\n",
       "      <td>56</td>\n",
       "      <td>0.017006</td>\n",
       "      <td>823</td>\n",
       "      <td>0.014108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>268</td>\n",
       "      <td>0.010718</td>\n",
       "      <td>353</td>\n",
       "      <td>0.011751</td>\n",
       "      <td>148</td>\n",
       "      <td>0.044944</td>\n",
       "      <td>769</td>\n",
       "      <td>0.013182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>842</td>\n",
       "      <td>0.033675</td>\n",
       "      <td>699</td>\n",
       "      <td>0.023270</td>\n",
       "      <td>90</td>\n",
       "      <td>0.027331</td>\n",
       "      <td>1631</td>\n",
       "      <td>0.027959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>375</td>\n",
       "      <td>0.014998</td>\n",
       "      <td>79</td>\n",
       "      <td>0.002630</td>\n",
       "      <td>49</td>\n",
       "      <td>0.014880</td>\n",
       "      <td>503</td>\n",
       "      <td>0.008622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    TopicID Topic Keywords Topic Summary  Western Count  Freq in Western  \\\n",
       "20       19                                         469         0.018757   \n",
       "13       12                                         792         0.031675   \n",
       "11       10                                         138         0.005519   \n",
       "15       14                                         268         0.010718   \n",
       "18       17                                         842         0.033675   \n",
       "17       16                                         375         0.014998   \n",
       "\n",
       "    Russian Count  Freq in Russian  MoscowTimes Count  Freq in MoscowTimes  \\\n",
       "20            674         0.022437                 22             0.006681   \n",
       "13           1289         0.042911                148             0.044944   \n",
       "11            629         0.020939                 56             0.017006   \n",
       "15            353         0.011751                148             0.044944   \n",
       "18            699         0.023270                 90             0.027331   \n",
       "17             79         0.002630                 49             0.014880   \n",
       "\n",
       "    Total Count  Total Freq  \n",
       "20         1165    0.019971  \n",
       "13         2229    0.038210  \n",
       "11          823    0.014108  \n",
       "15          769    0.013182  \n",
       "18         1631    0.027959  \n",
       "17          503    0.008622  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TopicID\tTopic Keywords\tTopic Summary\tWestern Count\tWestern Freq\tRussian Count\tRussian Freq\tMoscowTimes Count\tMoscowTimes Freq\tTotal Count\n",
    "topicCols = [col for col in df.columns if 'topic' in col]\n",
    "topicStatData = []\n",
    "for topicCol in topicCols:\n",
    "    topicRec = dict()\n",
    "    topicRec['TopicID'] = int(topicCol.split('_')[1])\n",
    "    topicRec['Topic Keywords'] = ''\n",
    "    topicRec['Topic Summary'] = ''\n",
    "    \n",
    "    topicRec['Western Count'] = df[df['category']=='western'][topicCol].sum()\n",
    "    topicRec['Freq in Western'] = df[df['category']=='western'][topicCol].sum()/df[df['category']=='western']['sentsInArticle'].sum()\n",
    "    \n",
    "    topicRec['Russian Count'] = df[df['category']=='russian'][topicCol].sum()\n",
    "    topicRec['Freq in Russian'] = df[df['category']=='russian'][topicCol].sum()/df[df['category']=='russian']['sentsInArticle'].sum()\n",
    "    \n",
    "    topicRec['MoscowTimes Count'] = df[df['category']=='moscowtimes'][topicCol].sum()\n",
    "    topicRec['Freq in MoscowTimes'] = df[df['category']=='moscowtimes'][topicCol].sum()/df[df['category']=='moscowtimes']['sentsInArticle'].sum()\n",
    "    \n",
    "    topicRec['Total Count'] = df[topicCol].sum()\n",
    "    topicRec['Total Freq'] = df[topicCol].sum()/df['sentsInArticle'].sum()\n",
    "    topicStatData.append(topicRec)\n",
    "    \n",
    "topicStats = pd.DataFrame.from_records(topicStatData)\n",
    "topicStats.sample(6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
